{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c818b02-2ae0-4686-b6e7-8c8ab1fe9bd9",
   "metadata": {},
   "source": [
    "# **Tutorial 6**\n",
    "\n",
    "### **Dask Pipeline Executor Advanced**\n",
    "\n",
    "In this tutorial, the objetive is present some specific details of the `DaskExecutorPipeline` and how you can configure everything that is related to this executor type.\n",
    "\n",
    "First, all the previous tutorials are creating an instance locally. This is the easiest way to test a Dask pipeline. Indeed, this is not the common practice when you are dealing with cluster in some cloud environment or HPC machines. Usually, developers persist data to split a huge data accross nodes/workers. So, let's see below how you can connect to a running Dask Cluster using our executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319841f-e97c-4c88-a73e-ce614acf46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dasf.pipeline.executors import DaskPipelineExecutor\n",
    "\n",
    "# machine 1.1.1.1: dask-scheduler\n",
    "# machine 2.2.2.2: dask-worker/dask-cuda-worker 1.1.1.1\n",
    "# machine 3.3.3.3: dask-worker/dask-cuda-worker 1.1.1.1\n",
    "# machine 4.4.4.4: dask-worker/dask-cuda-worker 1.1.1.1\n",
    "# machine 5.5.5.5: dask-worker/dask-cuda-worker 1.1.1.1\n",
    "dask = DaskPipelineExecutor(address=\"1.1.1.1\", port=8687)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e6149-2b81-4557-81c2-6de323c724c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "You should not be concerned if the target supports GPUs or not. Our executor automatically discovers what is the cluster type. On the other hand, Dasf does not support heterogeneous configurations like workers with GPUs and workers without GPUs simultaneously.\n",
    "\n",
    "The second example is now related to how you can limit a local Dask executor. When you provision a cloud environment with Dask workers, you already configure the resources limits. So, when you connect into it, you just need to run a pipeline. It is different when you initiate a local cluster. It automatically sets the maximum limits according the machine free resources. The example below is a local cluster with some customizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833115d-8ac2-4c28-9a2e-7a99c5008dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask = DaskPipelineExecutor(local=True, n_workers=4, threads_per_worker=1, processes=False,\n",
    "                            memory_limit='3GB', silence_logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b44699-54b6-4a34-b617-15f14162f868",
   "metadata": {},
   "source": [
    "The last option you can set using a Dask executor is the memory profiler. After you executing a Dask data graph, you can see the memory peak of each worker no matter if they are local or remote.\n",
    "\n",
    "You should be careful because if you run your pipeline multiple times, Dask cannot free memory properly and it can report more memory usage than it really was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb02a19-b57b-4a2d-bd12-480d0279ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask = DaskPipelineExecutor(profiler=\"memusage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d71cc-9840-48e5-9f75-9c38fac5c2a8",
   "metadata": {},
   "source": [
    "Another example to start a executor is using a PBS executor if you are running your code inside a cluster who manages jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cea66b-0c7f-4b8a-aea3-949a11483b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dasf.pipeline.executors import DaskPBSPipelineExecutor\n",
    "\n",
    "pbs = DaskPBSPipelineExecutor(cores=4, memory='20GB', processes=4, queue='myqueue', walltime='02:00:00', shebang='#!/bin/bash', n_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f0be1-d274-466f-9660-591c6d4b6bd6",
   "metadata": {},
   "source": [
    "The framework put this cluster requirement into the job queue, the code will wait until PBS executes the worker(s) and once it is ready you can run your code normally. This approach has only one problem, you cannot kick two jobs for different queues to use heterogeneous machines for instance. But this is the way PBS was implemented also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c9f97-df46-4110-be4d-f97f911850c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
