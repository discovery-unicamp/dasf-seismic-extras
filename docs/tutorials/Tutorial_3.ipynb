{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef41f05a",
   "metadata": {},
   "source": [
    "# **Tutorial 3**\n",
    "\n",
    "### **Specialized Operators**\n",
    "\n",
    "In this tutorial, we will present how a developer can create your own processing block to be used and connected into the pipeline. In the current stage of the framework, DASF Core has many specialized operators: `Transform`, `Fit`, `Preict`, `MappedTransform` and others.\n",
    "\n",
    "The `Transform` object is a generic operator that can be used to any purpose. Developers are free to implement the logic they want with some facilities provided by the object inheritance. The main idea of this operator is to extend a operation that transform the content of a dataset. The same idea behing machine learning algorithms implemented by Scikit Learn for example. This operator defines a method called `transform()` that does the magic.\n",
    "\n",
    "The `MappedTransform` applies a pre-defined function to each dask block if the geographic dependency is strongly connected. It also uses the `transform()` method exactly how the original class does. One example that we will present is the `mean()` method. It requires all data processed to calculate one value. It also requires some special logic to unify the results. The `MappedTransform` should change only that block with minimum effort.\n",
    "\n",
    "Let's create a simple example of plotting an attribute like we did for **Tutorial 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d87687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dasf_seismic.datasets import F3\n",
    "from dasf_seismic.attributes.complex_trace import Envelope\n",
    "from dasf_seismic.visualization import Plot2DIline\n",
    "from dasf.transforms import ExtractData\n",
    "\n",
    "dataset = F3(chunks={\"iline\": 5})\n",
    "\n",
    "extracted_data = ExtractData()\n",
    "\n",
    "envelope = Envelope()\n",
    "\n",
    "plot = Plot2DIline(name=\"Plot F3 block iline\", iline_index=100, swapaxes=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c1f1f",
   "metadata": {},
   "source": [
    "Now, we can create our cluster instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dasf.pipeline.executors import DaskPipelineExecutor\n",
    "\n",
    "dask = DaskPipelineExecutor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f022ef",
   "metadata": {},
   "source": [
    "We can create our simple pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52445f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dasf.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\"F3 Block plot pipeline\", executor=dask)\n",
    "\n",
    "pipeline.add(extracted_data, X=dataset) \\\n",
    "        .add(envelope, X=extracted_data) \\\n",
    "        .add(plot.plot, X=envelope) \\\n",
    "        .visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ce52f",
   "metadata": {},
   "source": [
    "Now, let's create our own operator to modify the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62431194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "from dasf.transforms import Transform\n",
    "\n",
    "class MyNormalize(Transform):\n",
    "    def _lazy_transform_gpu(self, X):\n",
    "        mean = da.mean(X)\n",
    "        std = da.std(X, ddof=0)\n",
    "        \n",
    "        return (X - mean)/std\n",
    "    \n",
    "    def _lazy_transform_cpu(self, X):\n",
    "        mean = da.mean(X)\n",
    "        std = da.std(X, ddof=0)\n",
    "        \n",
    "        return (X - mean)/std\n",
    "\n",
    "    def _transform_gpu(self, X):\n",
    "        mean = cp.mean(X)\n",
    "        std = cp.std(X, ddof=0)\n",
    "        \n",
    "        return (X - mean)/std\n",
    "    \n",
    "    def _transform_cpu(self, X):\n",
    "        mean = np.mean(X)\n",
    "        std = np.std(X, ddof=0)\n",
    "        \n",
    "        return (X - mean)/std\n",
    "\n",
    "    \n",
    "normalize = MyNormalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c6697",
   "metadata": {},
   "source": [
    "Notice here that we need to implement 4 methods to cover all possible architectures implemented by the default handlers. The Dask handlers usually manipulate `_lazy_*{cpu,gpu}` functions. On the other hand, local and simple pipelines do not require lazy operations and can be directly applied into the default target: the machine (CPU) or a GPU.\n",
    "\n",
    "Now, we can append this new block. Let's see what happens when we execute a new pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\"F3 Block plot pipeline\", executor=dask)\n",
    "\n",
    "pipeline.add(extracted_data, X=dataset) \\\n",
    "        .add(envelope, X=extracted_data) \\\n",
    "        .add(normalize, X=envelope) \\\n",
    "        .add(plot.plot, X=normalize) \\\n",
    "        .visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396730d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12cb789",
   "metadata": {},
   "source": [
    "As we can see, the output is the same, but now the data is normalized.\n",
    "\n",
    "Now, let's create a different example using `MappedTransform`. This operator processes each chunk of a dask data (Array or DataFrame) and it works only when the executor is **Dask**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6798d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a GPU example only. It is not necessary.\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "from dasf.transforms import MappedTransform\n",
    "\n",
    "\n",
    "cupy_raw_kernel = cp.RawKernel(r'''\n",
    "                extern \"C\" __global__\n",
    "                void limit_percent(const float *a, float *out,\n",
    "                                   float percent,\n",
    "                                   unsigned int nx, unsigned int ny,\n",
    "                                   unsigned int nz) {\n",
    "                    unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "                    unsigned int idy = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "                    unsigned int idz = threadIdx.z + blockIdx.z * blockDim.z;\n",
    "                    \n",
    "                    unsigned int center = 0;\n",
    "                    \n",
    "                    if ((idx >= 0 && idy >= 0 && idz >= 0) &&\n",
    "                        (idx < nx) && (idy < ny) && (idz < nz)) {\n",
    "                        center = ((ny * nz) * idx) + (idy * nz + idz);\n",
    "                        \n",
    "                        if (a[center] > percent) {\n",
    "                            out[center] = percent;\n",
    "                        } else {\n",
    "                            if (a[center] < -percent) {\n",
    "                                out[center] = -percent;\n",
    "                            } else {\n",
    "                                out[center] = a[center];\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ''', 'limit_percent')\n",
    "\n",
    "def limit_percent_block(block):\n",
    "    \"\"\"\n",
    "    A Simple function to calculate the 75% limits for a block.\n",
    "    \"\"\"\n",
    "    dimx = block.shape[0]\n",
    "    dimy = block.shape[1]\n",
    "    dimz = block.shape[2]\n",
    "\n",
    "    out = cp.zeros((dimz * dimy * dimx), dtype=cp.float32)\n",
    "    inp = cp.asarray(block.flatten(), dtype=cp.float32)\n",
    "\n",
    "    block_size = 10\n",
    "\n",
    "    grid = (int(np.ceil(dimx/block_size)),\n",
    "            int(np.ceil(dimy/block_size)),\n",
    "            int(np.ceil(dimz/block_size)),)\n",
    "    block = (block_size, block_size, block_size,)\n",
    "    \n",
    "    cupy_raw_kernel(grid, block, (inp, out, cp.float32(0.75),\n",
    "                                  cp.int32(dimx), cp.int32(dimy), cp.int32(dimz)))\n",
    "                    \n",
    "    return cp.asnumpy(out).reshape(dimx, dimy, dimz)\n",
    "                    \n",
    "\n",
    "class BlockPercent(MappedTransform):\n",
    "    def __init__(self):\n",
    "        super().__init__(function=limit_percent_block)\n",
    "        \n",
    "block_percent = BlockPercent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # The old executor set CUDA_VISIBLE_DEVICES to \"\", in order to avoid using GPU memory\n",
    "dask_gpu = DaskPipelineExecutor(use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e6b10",
   "metadata": {},
   "source": [
    "Now, we can append the new block and recreate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472aadf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\"F3 Block plot pipeline\", executor=dask_gpu)\n",
    "\n",
    "pipeline.add(extracted_data, X=dataset) \\\n",
    "        .add(envelope, X=extracted_data) \\\n",
    "        .add(normalize, X=envelope) \\\n",
    "        .add(block_percent, X=normalize) \\\n",
    "        .add(plot.plot, X=block_percent) \\\n",
    "        .visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81a85c",
   "metadata": {},
   "source": [
    "Let's test our new block implemented using a Raw Cupy Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c8a34",
   "metadata": {},
   "source": [
    "Now, let's reduce the size of each block to understand how we can take advantage of map/reduce capabilities. For this, we need to have a clear view of which shape each block will return.\n",
    "\n",
    "For the next example, our function will calculate the `mean()` of each block and return it into an array. Let's create a debug block to read the output either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_mean(block):\n",
    "    \"\"\"\n",
    "    A Simple function the mean of each block.\n",
    "    \"\"\"\n",
    "    return cp.mean(block)\n",
    "                    \n",
    "\n",
    "class BlockMean(MappedTransform):\n",
    "    def __init__(self):\n",
    "        super().__init__(function=block_mean, output_chunk=(1,))\n",
    "        \n",
    "class BlockDebug(Transform):\n",
    "    def transform(self, X):\n",
    "        # Aggregate debug function\n",
    "        print(\"Mean of the whole cube ---> \" + str(np.mean(X.compute())))\n",
    "\n",
    "        return X\n",
    "\n",
    "block_mean = BlockMean()\n",
    "debug_mean = BlockDebug()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e325a",
   "metadata": {},
   "source": [
    "Let's see what happens now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\"F3 Block plot pipeline\", executor=dask)\n",
    "\n",
    "pipeline.add(extracted_data, X=dataset) \\\n",
    "        .add(envelope, X=extracted_data) \\\n",
    "        .add(normalize, X=envelope) \\\n",
    "        .add(block_percent, X=normalize) \\\n",
    "        .add(block_mean, X=block_percent) \\\n",
    "        .add(debug_mean, X=block_mean) \\\n",
    "        .visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68361cbd",
   "metadata": {},
   "source": [
    "Time to test it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bfe495",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29cfb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
